{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "## Workflow Description\n",
    "\n",
    "This Jupyter Notebook document contains a workflow for stateful streaming analysis using Apache Spark and Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 0: Spark Session Initialization\n",
    "In this cell, we initialize the Spark session and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Statefull_Streaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Create Database\n",
    "This cell creates a database if it doesn't already exist in the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS orders.statefull_streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Define File Paths\n",
    "This cell defines the file paths for the Delta tables and checkpoint locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_TABLE =  \"s3://labdataset/delta/orders\"\n",
    "HIGH_VALUE_CHECKPOINT_LOCATION = \"s3://labdataset/delta/high_value_orders/checkpoint\"\n",
    "HIGH_VALUE_LOCATION = \"s3://labdataset/delta/high_value_orders/data\"\n",
    "ITEM_CUSTOMER_CHECKPOINT_LOCATION = \"s3://labdataset/delta/item_customer_count/checkpoint\"\n",
    "ITEM_CUSTOMER_LOCATION = \"s3://labdataset/delta/item_customer_count/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Reading Streaming Data\n",
    "This cell reads the streaming data from the Delta table and displays it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = spark.readStream.format(\"delta\").load(DELTA_TABLE)\n",
    "display(stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "#### Select the field we need for our statefull streaming counters : \n",
    "\n",
    "- Count orders where total amount > 100.\n",
    "- Count items per customerId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = stat_df.select(\n",
    "            col('batch_id'),\n",
    "            col('parsedValue.totalAmount').alias('totalAmount'),\n",
    "            col('parsedValue.customerId').alias('customerId'),\n",
    "            col('parsedValue.items').alias('items'),\n",
    "        )\n",
    "\n",
    "stat_df.createOrReplaceTempView(\"stat_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: First Counter Aggregation\n",
    "This cell performs the first counter aggregation by counting the number of high-value orders where the total amount is greater than 4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_orders_query = \"\"\"\n",
    "SELECT COUNT(*) AS highValueOrderCount\n",
    "FROM stat_view\n",
    "WHERE totalAmount > 4000\n",
    "\"\"\"\n",
    "\n",
    "high_value_orders_df = spark.sql(high_value_orders_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Save First Counter Output\n",
    "This cell saves the output of the first counter aggregation into an S3 location using Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_orders_output_query = high_value_orders_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", HIGH_VALUE_CHECKPOINT_LOCATION) \\\n",
    "    .option(\"path\", HIGH_VALUE_LOCATION) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Second Counter Aggregation\n",
    "This cell performs the second counter aggregation by counting the number of items per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_per_customer_query = \"\"\"\n",
    "SELECT customerId, COUNT(*) AS itemCount\n",
    "FROM (\n",
    "    SELECT customerId, EXPLODE(items) AS item\n",
    "    FROM stat_view\n",
    ") exploded_view\n",
    "GROUP BY customerId\n",
    "\"\"\"\n",
    "\n",
    "items_per_customer_df = spark.sql(items_per_customer_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Save Second Counter Output\n",
    "This cell saves the output of the second counter aggregation into an S3 location using Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_per_customer_output_query  = items_per_customer_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", ITEM_CUSTOMER_CHECKPOINT_LOCATION) \\\n",
    "    .option(\"path\", ITEM_CUSTOMER_LOCATION) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 14: Await Termination\n",
    "This cell waits for the streaming queries to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_orders_output_query.awaitTermination()\n",
    "items_per_customer_output_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
